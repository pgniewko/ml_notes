{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350f2916-90ff-406e-9b51-e77e2ee95b48",
   "metadata": {},
   "source": [
    "Data downloaded from (access data 09/21/2025): https://www.uniprot.org/help/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ee8a95-491a-4d1e-b5d1-9c5dced2111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "from typing import Callable, Any, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a38cf-1686-42a8-802e-052f1022220c",
   "metadata": {},
   "source": [
    "## Reproducibility & speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8613a0-ae8b-43ec-93fb-5086dca1b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb4f9b-762a-4285-959a-d7b030406ccb",
   "metadata": {},
   "source": [
    "## Training & eval helpers &  Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cb54a2-89bd-4da2-b45d-fd9f58c06965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(X: torch.Tensor, batch_size: int = 256, num_workers: int = 2):\n",
    "    N = len(X)\n",
    "    n_train = int(0.8 * N)\n",
    "    n_val = int(0.1 * N)\n",
    "    n_test = N - n_train - n_val\n",
    "    ds = TensorDataset(X)\n",
    "    train_set, val_set, test_set = random_split(\n",
    "        ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    def make_loader(dset, shuffle=False):\n",
    "        return DataLoader(\n",
    "            dset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    return make_loader(train_set, True), make_loader(val_set), make_loader(test_set)\n",
    "\n",
    "\n",
    "def load_h5_embeddings(path: str) -> torch.Tensor:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        protein_ids = list(f.keys())\n",
    "        embeddings = [f[pid][()] for pid in protein_ids]\n",
    "    X = torch.tensor(np.stack(embeddings), dtype=torch.float32)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c9499-f137-4c4b-9385-ede97c79ab2e",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e7f8e2-d8cb-4a1d-84fd-6cf154cfe46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mean_squared_error(\n",
    "    reconstruction: torch.Tensor,\n",
    "    original_input: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    return (\n",
    "        ((reconstruction - original_input) ** 2).mean(dim=1) / (original_input**2).mean(dim=1)\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "def normalized_L1_loss(\n",
    "    latent_activations: torch.Tensor,\n",
    "    original_input: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    return (latent_activations.abs().sum(dim=1) / original_input.norm(dim=1)).mean()\n",
    "\n",
    "\n",
    "def autoencoder_loss(\n",
    "    reconstruction: torch.Tensor,\n",
    "    original_input: torch.Tensor,\n",
    "    latent_activations: torch.Tensor,\n",
    "    l1_weight: float,\n",
    ") -> torch.Tensor:\n",
    "    return (\n",
    "        normalized_mean_squared_error(reconstruction, original_input)\n",
    "        + normalized_L1_loss(latent_activations, original_input) * l1_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483d0b18-68eb-45b8-ac23-76bd308a0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP = True  # turn on mixed precision\n",
    "GRAD_CLIP_NORM = 1.0  # set to None to disable\n",
    "USE_COSINE = True  # cosine LR with warmup\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Non-trainable LayerNorm\n",
    "# =========================\n",
    "def layer_norm_no_affine(\n",
    "    x: torch.Tensor, eps: float = 1e-5\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Non-trainable LayerNorm over the last dim; returns (x_norm, mean, std).\n",
    "    \"\"\"\n",
    "    mu = x.mean(dim=-1, keepdim=True)\n",
    "    x_center = x - mu\n",
    "    var = (x_center ** 2).mean(dim=-1, keepdim=True)\n",
    "    std = torch.sqrt(var + eps)\n",
    "    return x_center / std, mu, std\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Weight tying wrapper\n",
    "# =========================\n",
    "class TiedTranspose(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert self.linear.bias is None\n",
    "        return F.linear(x, self.linear.weight.t(), None)\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.Tensor:\n",
    "        return self.linear.weight.t()\n",
    "\n",
    "    @property\n",
    "    def bias(self) -> torch.Tensor:\n",
    "        return self.linear.bias\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sparse activations\n",
    "# =========================\n",
    "class TopK(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-k sparse activation.\n",
    "\n",
    "    Keeps the k largest values per row (last dimension) and sets all\n",
    "    others to zero. Optionally applies a post-activation function\n",
    "    (default: ReLU) to the selected values before reinserting them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int, postact_fn: Callable = nn.ReLU()) -> None:\n",
    "        super().__init__()\n",
    "        self.k = int(k)\n",
    "        self.postact_fn = postact_fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [..., d].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of the same shape as x, with only the top-k values\n",
    "            (per last dimension) kept and transformed by postact_fn;\n",
    "            all other entries are zero.\n",
    "        \"\"\"\n",
    "        topk = torch.topk(x, k=self.k, dim=-1)\n",
    "        values = self.postact_fn(topk.values)\n",
    "        result = torch.zeros_like(x)\n",
    "        result.scatter_(-1, topk.indices, values)\n",
    "        return result\n",
    "\n",
    "\n",
    "def _sample_gumbel(shape, device, eps=1e-20):\n",
    "    \"\"\"\n",
    "    Draw i.i.d. samples from a standard Gumbel(0, 1) distribution.\n",
    "\n",
    "    Args:\n",
    "        shape: output tensor shape.\n",
    "        device: torch device to place the samples on.\n",
    "        eps: small constant for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of Gumbel noise with the given shape.\n",
    "    \"\"\"\n",
    "    u = torch.rand(shape, device=device)\n",
    "    return -torch.log(-torch.log(u + eps) + eps)\n",
    "\n",
    "\n",
    "class GumbelTopK(nn.Module):\n",
    "    \"\"\"Top-k with Gumbel; tau linearly anneals to tau_end then becomes hard Top-k.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        postact_fn: Callable = nn.ReLU(),\n",
    "        tau_start: float = 1.0,\n",
    "        tau_end: float = 1e-4,\n",
    "        anneal_steps: int = 100,\n",
    "        use_straight_through: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert k >= 1 and tau_start > 0 and tau_end > 0 and anneal_steps >= 1\n",
    "        self.k = int(k)\n",
    "        self.postact_fn = postact_fn\n",
    "        self.tau_start = float(tau_start)\n",
    "        self.tau_end = float(tau_end)\n",
    "        self.anneal_steps = int(anneal_steps)\n",
    "        self.use_straight_through = bool(use_straight_through)\n",
    "        self.register_buffer(\"step\", torch.zeros((), dtype=torch.long))\n",
    "        self.register_buffer(\"tau\", torch.tensor(self.tau_start, dtype=torch.float))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_tau(self):\n",
    "        \"\"\" Linearly anneals `tau` from tau_start â†’ tau_end over anneal_steps.\n",
    "        \n",
    "        Clamps at tau_end.\n",
    "        \"\"\"\n",
    "        s = int(self.step.item())\n",
    "        frac = min(s, self.anneal_steps) / self.anneal_steps\n",
    "        new_tau = self.tau_start + (self.tau_end - self.tau_start) * frac\n",
    "        self.tau.fill_(max(self.tau_end, float(new_tau)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        d = x.size(-1)\n",
    "        assert self.k <= d, f\"k={self.k} cannot exceed last dimension {d}\"\n",
    "        values = self.postact_fn(x)\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.step.add_(1)\n",
    "                self._update_tau()\n",
    "\n",
    "            # Once tau ~ tau_end, deterministic top-k on raw scores\n",
    "            if self.tau.item() <= self.tau_end + 1e-12:\n",
    "                topk = torch.topk(x, k=self.k, dim=-1)\n",
    "                mask = torch.zeros_like(x).scatter(-1, topk.indices, 1.0)\n",
    "                return values * mask\n",
    "\n",
    "            # Stochastic Gumbel Top-k with STE\n",
    "            g = _sample_gumbel(x.shape, device=x.device)\n",
    "            logits = (x + g) / self.tau.clamp(min=1e-8)\n",
    "            soft = F.softmax(logits, dim=-1)\n",
    "            topk = torch.topk(logits, k=self.k, dim=-1)\n",
    "            hard_mask = torch.zeros_like(x).scatter(-1, topk.indices, 1.0)\n",
    "            mask = (hard_mask - soft).detach() + soft if self.use_straight_through else soft\n",
    "            return values * mask\n",
    "        else:\n",
    "            topk = torch.topk(x, k=self.k, dim=-1)\n",
    "            mask = torch.zeros_like(x).scatter(-1, topk.indices, 1.0)\n",
    "            return values * mask\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sparse Autoencoder\n",
    "# =========================\n",
    "class SparseAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    latents = activation(encoder(x - pre_bias) + latent_bias)\n",
    "    recons  = decoder(latents) + pre_bias\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_latents: int,\n",
    "        n_inputs: int,\n",
    "        activation: Callable = nn.ReLU(),\n",
    "        tied: bool = True,\n",
    "        normalize: bool = False,\n",
    "        init_row_norm_decoder: bool = True,  # row-normalize decoder at init (stabilizes early training)\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.pre_bias = nn.Parameter(torch.zeros(n_inputs))\n",
    "\n",
    "        # ---- Encoder (no bias); Kaiming/He-init like tricks ----\n",
    "        self.encoder: nn.Linear = nn.Linear(n_inputs, n_latents, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.latent_bias = nn.Parameter(torch.zeros(n_latents))\n",
    "\n",
    "        # ---- Decoder: tied or free ----\n",
    "        if tied:\n",
    "            self.decoder: nn.Module = TiedTranspose(self.encoder)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(n_latents, n_inputs, bias=False)\n",
    "            nn.init.kaiming_uniform_(self.decoder.weight, a=math.sqrt(5))\n",
    "\n",
    "        # Optionally start decoder rows normalized (OpenAI/saefarer friendly)\n",
    "        if isinstance(self.decoder, nn.Linear) and init_row_norm_decoder:\n",
    "            with torch.no_grad():\n",
    "                self.decoder.weight.data = F.normalize(self.decoder.weight.data, p=2, dim=1)\n",
    "        elif isinstance(self.decoder, TiedTranspose) and init_row_norm_decoder:\n",
    "            with torch.no_grad():\n",
    "                W = self.encoder.weight.data\n",
    "                self.encoder.weight.data = F.normalize(W, p=2, dim=0)  # normalize columns so decoder rows are unit\n",
    "\n",
    "        self.activation = activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Runtime stats (optional; mirrors saefarer/OpenAI idea)\n",
    "        self.register_buffer(\"stats_last_nonzero\", torch.zeros(n_latents, dtype=torch.long))\n",
    "        self.register_buffer(\"latents_activation_frequency\", torch.ones(n_latents, dtype=torch.float))\n",
    "        self.register_buffer(\"latents_mean_square\", torch.zeros(n_latents, dtype=torch.float))\n",
    "\n",
    "    def encode_pre_act(self, x: torch.Tensor, latent_slice: slice = slice(None)) -> torch.Tensor:\n",
    "        x = x - self.pre_bias\n",
    "        latents_pre_act = F.linear(\n",
    "            x, self.encoder.weight[latent_slice], self.latent_bias[latent_slice]\n",
    "        )\n",
    "        return latents_pre_act\n",
    "\n",
    "    def preprocess(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        if not self.normalize:\n",
    "            return x, {}\n",
    "        x_norm, mu, std = layer_norm_no_affine(x)\n",
    "        return x_norm, {\"mu\": mu, \"std\": std}\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        x, info = self.preprocess(x)\n",
    "        return self.activation(self.encode_pre_act(x)), info\n",
    "\n",
    "    def decode(self, latents: torch.Tensor, info: Dict[str, Any] | None = None) -> torch.Tensor:\n",
    "        ret = self.decoder(latents) + self.pre_bias\n",
    "        if self.normalize:\n",
    "            assert info is not None\n",
    "            ret = ret * info[\"std\"] + info[\"mu\"]\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_proc, info = self.preprocess(x)\n",
    "        latents_pre_act = self.encode_pre_act(x_proc)\n",
    "        latents = self.activation(latents_pre_act)\n",
    "        recons = self.decode(latents, info)\n",
    "\n",
    "        # Update idle stats like OpenAI/saefarer\n",
    "        self.stats_last_nonzero *= (latents == 0).all(dim=0).long()\n",
    "        self.stats_last_nonzero += 1\n",
    "\n",
    "        return latents_pre_act, latents, recons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf7563-6a07-48d8-9d38-e04555af91a0",
   "metadata": {},
   "source": [
    "## Train, and eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07879a67-52da-4b13-ad70-b9fcb2b311bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_epoch(model, loader, l1_weight: float) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    losses, mses = [], []\n",
    "    for (x,) in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        latents_pre, latents, recons = model(x)\n",
    "        loss = autoencoder_loss(recons, x, latents, l1_weight)\n",
    "        mse = normalized_mean_squared_error(recons, x)\n",
    "        losses.append(loss.item())\n",
    "        mses.append(mse.item())\n",
    "    return {\"loss\": float(np.mean(losses)), \"nmse\": float(np.mean(mses))}\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    *,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 1e-3,\n",
    "    l1_weight: float = 0.0,\n",
    "    warmup_steps: int = 0,\n",
    "):\n",
    "    model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    if USE_COSINE:\n",
    "        total_steps = epochs * max(1, len(train_loader))\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return max(1e-6, (step + 1) / max(1, warmup_steps))\n",
    "            prog = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1 + math.cos(math.pi * min(1.0, max(0.0, prog))))\n",
    "        sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "    else:\n",
    "        sched = None\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [], \"train_nmse\": [],\n",
    "        \"val_loss\": [], \"val_nmse\": [],\n",
    "        \"test_loss\": [], \"test_nmse\": [],\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_losses, train_mses = [], []\n",
    "        for (x,) in train_loader:\n",
    "            x = x.to(DEVICE, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                latents_pre, latents, recons = model(x)\n",
    "                loss_total = autoencoder_loss(recons, x, latents, l1_weight)\n",
    "                nmse = normalized_mean_squared_error(recons, x)\n",
    "\n",
    "            scaler.scale(loss_total).backward()\n",
    "\n",
    "            if GRAD_CLIP_NORM is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            if sched is not None:\n",
    "                sched.step()\n",
    "            global_step += 1\n",
    "\n",
    "            train_losses.append(loss_total.item())\n",
    "            train_mses.append(nmse.item())\n",
    "\n",
    "        # Epoch-end eval\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "        train_nmse = float(np.mean(train_mses))\n",
    "        val_metrics = evaluate_epoch(model, val_loader, l1_weight)\n",
    "        test_metrics = evaluate_epoch(model, test_loader, l1_weight)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_nmse\"].append(train_nmse)\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_nmse\"].append(val_metrics[\"nmse\"])\n",
    "        history[\"test_loss\"].append(test_metrics[\"loss\"])\n",
    "        history[\"test_nmse\"].append(test_metrics[\"nmse\"])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"train: loss {train_loss:.4f}, nmse {train_nmse:.4f} | \"\n",
    "            f\"val: loss {val_metrics['loss']:.4f}, nmse {val_metrics['nmse']:.4f} | \"\n",
    "            f\"test: loss {test_metrics['loss']:.4f}, nmse {test_metrics['nmse']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfbd37-49f9-4de0-a18c-7d651c09bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/3r1nbv8s4qx7qvtcgk18pwhw0000gn/T/ipykernel_8559/362259851.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/var/folders/zs/3r1nbv8s4qx7qvtcgk18pwhw0000gn/T/ipykernel_8559/362259851.py:54: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train: loss 1.4647, nmse 1.4607 | val: loss 0.8309, nmse 0.8286 | test: loss 0.8236, nmse 0.8214\n",
      "Epoch 02 | train: loss 0.6381, nmse 0.6351 | val: loss 0.5290, nmse 0.5255 | test: loss 0.5179, nmse 0.5144\n",
      "Epoch 03 | train: loss 0.4331, nmse 0.4290 | val: loss 0.3561, nmse 0.3513 | test: loss 0.3474, nmse 0.3426\n",
      "Epoch 04 | train: loss 0.2907, nmse 0.2850 | val: loss 0.2439, nmse 0.2371 | test: loss 0.2368, nmse 0.2300\n",
      "Epoch 05 | train: loss 0.2094, nmse 0.2015 | val: loss 0.1898, nmse 0.1810 | test: loss 0.1836, nmse 0.1747\n",
      "Epoch 06 | train: loss 0.1698, nmse 0.1601 | val: loss 0.1614, nmse 0.1510 | test: loss 0.1557, nmse 0.1452\n",
      "Epoch 07 | train: loss 0.1478, nmse 0.1368 | val: loss 0.1444, nmse 0.1330 | test: loss 0.1390, nmse 0.1275\n",
      "Epoch 08 | train: loss 0.1343, nmse 0.1224 | val: loss 0.1330, nmse 0.1209 | test: loss 0.1278, nmse 0.1156\n",
      "Epoch 09 | train: loss 0.1247, nmse 0.1122 | val: loss 0.1253, nmse 0.1126 | test: loss 0.1204, nmse 0.1077\n",
      "Epoch 10 | train: loss 0.1179, nmse 0.1050 | val: loss 0.1193, nmse 0.1062 | test: loss 0.1146, nmse 0.1015\n",
      "Epoch 11 | train: loss 0.1126, nmse 0.0994 | val: loss 0.1146, nmse 0.1013 | test: loss 0.1101, nmse 0.0967\n",
      "Epoch 12 | train: loss 0.1085, nmse 0.0950 | val: loss 0.1110, nmse 0.0975 | test: loss 0.1066, nmse 0.0930\n",
      "Epoch 13 | train: loss 0.1053, nmse 0.0916 | val: loss 0.1079, nmse 0.0942 | test: loss 0.1037, nmse 0.0899\n",
      "Epoch 14 | train: loss 0.1026, nmse 0.0887 | val: loss 0.1056, nmse 0.0918 | test: loss 0.1015, nmse 0.0876\n",
      "Epoch 15 | train: loss 0.1005, nmse 0.0865 | val: loss 0.1037, nmse 0.0898 | test: loss 0.0997, nmse 0.0857\n",
      "Epoch 16 | train: loss 0.0987, nmse 0.0846 | val: loss 0.1021, nmse 0.0881 | test: loss 0.0982, nmse 0.0841\n",
      "Epoch 17 | train: loss 0.0973, nmse 0.0832 | val: loss 0.1009, nmse 0.0868 | test: loss 0.0970, nmse 0.0828\n",
      "Epoch 18 | train: loss 0.0962, nmse 0.0819 | val: loss 0.0999, nmse 0.0857 | test: loss 0.0960, nmse 0.0818\n",
      "Epoch 19 | train: loss 0.0953, nmse 0.0810 | val: loss 0.0992, nmse 0.0850 | test: loss 0.0953, nmse 0.0811\n",
      "Epoch 20 | train: loss 0.0946, nmse 0.0803 | val: loss 0.0986, nmse 0.0844 | test: loss 0.0948, nmse 0.0805\n",
      "Epoch 21 | train: loss 0.0940, nmse 0.0797 | val: loss 0.0982, nmse 0.0840 | test: loss 0.0944, nmse 0.0801\n",
      "Epoch 22 | train: loss 0.0937, nmse 0.0793 | val: loss 0.0979, nmse 0.0837 | test: loss 0.0942, nmse 0.0798\n",
      "Epoch 23 | train: loss 0.0934, nmse 0.0791 | val: loss 0.0978, nmse 0.0835 | test: loss 0.0940, nmse 0.0797\n",
      "Epoch 24 | train: loss 0.0933, nmse 0.0789 | val: loss 0.0977, nmse 0.0834 | test: loss 0.0940, nmse 0.0796\n",
      "Epoch 25 | train: loss 0.0932, nmse 0.0788 | val: loss 0.0977, nmse 0.0834 | test: loss 0.0939, nmse 0.0796\n",
      "Epoch 01 | train: loss 1.5053, nmse 1.5053 | val: loss 1.0568, nmse 1.0568 | test: loss 1.0521, nmse 1.0521\n",
      "Epoch 02 | train: loss 0.8657, nmse 0.8657 | val: loss 0.6448, nmse 0.6448 | test: loss 0.6325, nmse 0.6325\n",
      "Epoch 03 | train: loss 0.5618, nmse 0.5618 | val: loss 0.4770, nmse 0.4770 | test: loss 0.4652, nmse 0.4652\n",
      "Epoch 04 | train: loss 0.3953, nmse 0.3953 | val: loss 0.3360, nmse 0.3360 | test: loss 0.3266, nmse 0.3266\n",
      "Epoch 05 | train: loss 0.2915, nmse 0.2915 | val: loss 0.2639, nmse 0.2639 | test: loss 0.2566, nmse 0.2566\n",
      "Epoch 06 | train: loss 0.2413, nmse 0.2413 | val: loss 0.2300, nmse 0.2300 | test: loss 0.2226, nmse 0.2226\n",
      "Epoch 07 | train: loss 0.2149, nmse 0.2149 | val: loss 0.2089, nmse 0.2089 | test: loss 0.2021, nmse 0.2021\n",
      "Epoch 08 | train: loss 0.1973, nmse 0.1973 | val: loss 0.1946, nmse 0.1946 | test: loss 0.1882, nmse 0.1882\n",
      "Epoch 09 | train: loss 0.1852, nmse 0.1852 | val: loss 0.1845, nmse 0.1845 | test: loss 0.1783, nmse 0.1783\n",
      "Epoch 10 | train: loss 0.1765, nmse 0.1765 | val: loss 0.1772, nmse 0.1772 | test: loss 0.1712, nmse 0.1712\n",
      "Epoch 11 | train: loss 0.1704, nmse 0.1704 | val: loss 0.1723, nmse 0.1723 | test: loss 0.1662, nmse 0.1662\n",
      "Epoch 12 | train: loss 0.1660, nmse 0.1660 | val: loss 0.1683, nmse 0.1683 | test: loss 0.1625, nmse 0.1625\n",
      "Epoch 13 | train: loss 0.1623, nmse 0.1623 | val: loss 0.1653, nmse 0.1653 | test: loss 0.1594, nmse 0.1594\n",
      "Epoch 14 | train: loss 0.1593, nmse 0.1593 | val: loss 0.1626, nmse 0.1626 | test: loss 0.1569, nmse 0.1569\n",
      "Epoch 15 | train: loss 0.1569, nmse 0.1569 | val: loss 0.1606, nmse 0.1606 | test: loss 0.1549, nmse 0.1549\n",
      "Epoch 16 | train: loss 0.1549, nmse 0.1549 | val: loss 0.1588, nmse 0.1588 | test: loss 0.1532, nmse 0.1532\n",
      "Epoch 17 | train: loss 0.1533, nmse 0.1533 | val: loss 0.1576, nmse 0.1576 | test: loss 0.1520, nmse 0.1520\n",
      "Epoch 18 | train: loss 0.1520, nmse 0.1520 | val: loss 0.1565, nmse 0.1565 | test: loss 0.1510, nmse 0.1510\n",
      "Epoch 19 | train: loss 0.1510, nmse 0.1510 | val: loss 0.1557, nmse 0.1557 | test: loss 0.1502, nmse 0.1502\n",
      "Epoch 20 | train: loss 0.1503, nmse 0.1503 | val: loss 0.1551, nmse 0.1551 | test: loss 0.1496, nmse 0.1496\n",
      "Epoch 21 | train: loss 0.1496, nmse 0.1496 | val: loss 0.1548, nmse 0.1548 | test: loss 0.1492, nmse 0.1492\n",
      "Epoch 22 | train: loss 0.1492, nmse 0.1492 | val: loss 0.1545, nmse 0.1545 | test: loss 0.1490, nmse 0.1490\n",
      "Epoch 23 | train: loss 0.1490, nmse 0.1490 | val: loss 0.1544, nmse 0.1544 | test: loss 0.1488, nmse 0.1488\n",
      "Epoch 24 | train: loss 0.1488, nmse 0.1488 | val: loss 0.1543, nmse 0.1543 | test: loss 0.1488, nmse 0.1488\n",
      "Epoch 25 | train: loss 0.1488, nmse 0.1488 | val: loss 0.1543, nmse 0.1543 | test: loss 0.1488, nmse 0.1488\n",
      "Epoch 01 | train: loss 1.4144, nmse 1.4144 | val: loss 1.0532, nmse 1.0532 | test: loss 1.0481, nmse 1.0481\n",
      "Epoch 02 | train: loss 1.0045, nmse 1.0045 | val: loss 0.9823, nmse 0.9823 | test: loss 0.9806, nmse 0.9806\n",
      "Epoch 03 | train: loss 0.9639, nmse 0.9639 | val: loss 0.9477, nmse 0.9477 | test: loss 0.9459, nmse 0.9459\n",
      "Epoch 04 | train: loss 0.9280, nmse 0.9280 | val: loss 0.9135, nmse 0.9135 | test: loss 0.9110, nmse 0.9110\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Plotting\n",
    "# =========================\n",
    "def plot_summary(histA, histB, histC, title=\"SAE variants comparison\"):\n",
    "    \"\"\"\n",
    "    Builds a 2x3 grid: rows = [Loss, NMSE], cols = [Train, Val, Test]\n",
    "    Bars = [ReLU+L1, TopK, GumbelTopK] using the final-epoch values.\n",
    "    \"\"\"\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    models = [\"ReLU+L1\", \"TopK=64\", \"GumbelTopK=64\"]\n",
    "\n",
    "    # Final epoch values\n",
    "    final = {\n",
    "        \"ReLU+L1\": {\n",
    "            \"loss\": [histA[\"train_loss\"][-1], histA[\"val_loss\"][-1], histA[\"test_loss\"][-1]],\n",
    "            \"nmse\": [histA[\"train_nmse\"][-1], histA[\"val_nmse\"][-1], histA[\"test_nmse\"][-1]],\n",
    "        },\n",
    "        \"TopK=64\": {\n",
    "            \"loss\": [histB[\"train_loss\"][-1], histB[\"val_loss\"][-1], histB[\"test_loss\"][-1]],\n",
    "            \"nmse\": [histB[\"train_nmse\"][-1], histB[\"val_nmse\"][-1], histB[\"test_nmse\"][-1]],\n",
    "        },\n",
    "        \"GumbelTopK=64\": {\n",
    "            \"loss\": [histC[\"train_loss\"][-1], histC[\"val_loss\"][-1], histC[\"test_loss\"][-1]],\n",
    "            \"nmse\": [histC[\"train_nmse\"][-1], histC[\"val_nmse\"][-1], histC[\"test_nmse\"][-1]],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 6), constrained_layout=True)\n",
    "    width = 0.25\n",
    "    x = np.arange(len(models))\n",
    "\n",
    "    for col, split in enumerate(splits):\n",
    "        # Loss row\n",
    "        ax = axes[0, col]\n",
    "        vals = [final[m][\"loss\"][col] for m in models]\n",
    "        ax.bar(x, vals, width)\n",
    "        ax.set_title(f\"{split.capitalize()} Loss\")\n",
    "        ax.set_xticks(x, models, rotation=20)\n",
    "        ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        # NMSE row\n",
    "        ax = axes[1, col]\n",
    "        vals = [final[m][\"nmse\"][col] for m in models]\n",
    "        ax.bar(x, vals, width)\n",
    "        ax.set_title(f\"{split.capitalize()} NMSE\")\n",
    "        ax.set_xticks(x, models, rotation=20)\n",
    "        ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    # ---- Load data ----\n",
    "    file_path = \"./data/per-protein.h5\"\n",
    "    X = load_h5_embeddings(file_path)  # [N, D]\n",
    "    n_inputs = X.shape[1]\n",
    "\n",
    "    # ---- Dataloaders ----\n",
    "    train_loader, val_loader, test_loader = build_loaders(X, batch_size=256, num_workers=2)\n",
    "\n",
    "    # ---- Common config ----\n",
    "    epochs = 25\n",
    "    lr = 1e-3\n",
    "    warmup_steps = 200\n",
    "\n",
    "    # ---- Model A: ReLU SAE (MSE + L1), no TopK ----\n",
    "    model_a = SparseAutoEncoder(\n",
    "        n_latents=256,\n",
    "        n_inputs=n_inputs,\n",
    "        activation=nn.ReLU(),\n",
    "        tied=True,               # initialize decoder as encoderáµ€\n",
    "        normalize=False,         # no LN in this variant\n",
    "        init_row_norm_decoder=True,\n",
    "    )\n",
    "    histA = train_model(\n",
    "        model_a, train_loader, val_loader, test_loader,\n",
    "        epochs=epochs, lr=lr, l1_weight=1e-3, warmup_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    # ---- Model B: TopK=64 (MSE only) ----\n",
    "    model_b = SparseAutoEncoder(\n",
    "        n_latents=256,\n",
    "        n_inputs=n_inputs,\n",
    "        activation=TopK(k=64),\n",
    "        tied=True,\n",
    "        normalize=True,          # good practice with sparse activations\n",
    "        init_row_norm_decoder=True,\n",
    "    )\n",
    "    histB = train_model(\n",
    "        model_b, train_loader, val_loader, test_loader,\n",
    "        epochs=epochs, lr=lr, l1_weight=0.0, warmup_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    # ---- Model C: GumbelTopK=64 (MSE only) ----\n",
    "    model_c = SparseAutoEncoder(\n",
    "        n_latents=256,\n",
    "        n_inputs=n_inputs,\n",
    "        activation=GumbelTopK(k=64, tau_start=1.0, tau_end=1e-8, anneal_steps=10_000),\n",
    "        tied=True,\n",
    "        normalize=True,\n",
    "        init_row_norm_decoder=True,\n",
    "    )\n",
    "    histC = train_model(\n",
    "        model_c, train_loader, val_loader, test_loader,\n",
    "        epochs=epochs, lr=lr, l1_weight=0.0, warmup_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    # ---- Figure: compare final metrics ----\n",
    "    plot_summary(histA, histB, histC, title=\"Sparse Autoencoder Variants (Loss & NMSE)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
