{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd59fcc-151e-4ed4-a933-22391c83ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "\n",
    "    def forward(self, z_e):\n",
    "        # z_e: [B, D]\n",
    "        flat_input = z_e.view(-1, self.embedding_dim)\n",
    "        # Compute distances to codebook entries\n",
    "        distances2 = (\n",
    "            torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embeddings.weight ** 2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self.embeddings.weight.t())\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances2, dim=1)\n",
    "        quantized = self.embeddings(encoding_indices).view(z_e.shape)\n",
    "        # Codebook and commitment losses\n",
    "        codebook_loss = F.mse_loss(quantized.detach(), z_e)\n",
    "        commitment_loss = F.mse_loss(quantized, z_e.detach())\n",
    "        loss = codebook_loss + self.commitment_cost * commitment_loss\n",
    "        # Straight-through estimator\n",
    "        quantized = z_e + (quantized - z_e).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_dim, hidden_dim, latent_dim)\n",
    "        self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, vq_loss, encoding_indices = self.vq(z_e)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        return x_recon, vq_loss, recon_loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e76502-6f8f-4c33-bcfd-ceb5796bb4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 1.0836589336395264\n",
      "VQ loss (commitment + codebook): 0.08529667556285858\n",
      "Total loss:1.1689555644989014\n",
      "Encoding indices (codebook ids used): tensor([  0,  13,  27,   3,  63, 115,  64,   2])\n",
      "\n",
      "Sampled embeddings: tensor([[ 0.0021, -0.0006,  0.0030, -0.0030, -0.0055,  0.0051,  0.0027,  0.0025,\n",
      "         -0.0011, -0.0010, -0.0070, -0.0073, -0.0061, -0.0044, -0.0062,  0.0070],\n",
      "        [ 0.0013, -0.0036,  0.0056, -0.0037, -0.0001,  0.0076,  0.0014,  0.0066,\n",
      "          0.0077, -0.0047,  0.0075, -0.0028,  0.0022,  0.0040, -0.0010,  0.0009],\n",
      "        [-0.0062,  0.0046,  0.0078, -0.0004, -0.0036,  0.0034,  0.0018, -0.0010,\n",
      "         -0.0067, -0.0063,  0.0059, -0.0013, -0.0059, -0.0045, -0.0014,  0.0050],\n",
      "        [ 0.0013,  0.0076, -0.0018, -0.0040,  0.0054,  0.0004,  0.0076,  0.0044,\n",
      "         -0.0009, -0.0040, -0.0014,  0.0003,  0.0069,  0.0044,  0.0029,  0.0037]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Generated samples: tensor([[-0.1059,  0.0553,  0.2098, -0.0345, -0.0691, -0.1438, -0.1118,  0.1338,\n",
      "          0.0380, -0.0828,  0.0158, -0.1189,  0.1218,  0.1808, -0.0243,  0.0331,\n",
      "         -0.0575,  0.0452,  0.0192, -0.0053, -0.0950,  0.0849, -0.0655, -0.1469,\n",
      "          0.0541,  0.0221, -0.1137,  0.1709, -0.0962,  0.1323,  0.1399, -0.0364],\n",
      "        [-0.1066,  0.0517,  0.2109, -0.0319, -0.0714, -0.1448, -0.1130,  0.1332,\n",
      "          0.0377, -0.0828,  0.0157, -0.1179,  0.1218,  0.1825, -0.0240,  0.0297,\n",
      "         -0.0589,  0.0458,  0.0168, -0.0040, -0.0969,  0.0819, -0.0631, -0.1458,\n",
      "          0.0492,  0.0195, -0.1136,  0.1697, -0.0968,  0.1340,  0.1429, -0.0369],\n",
      "        [-0.1041,  0.0540,  0.2091, -0.0349, -0.0686, -0.1438, -0.1135,  0.1335,\n",
      "          0.0382, -0.0816,  0.0150, -0.1198,  0.1213,  0.1822, -0.0242,  0.0315,\n",
      "         -0.0578,  0.0455,  0.0174, -0.0049, -0.0968,  0.0847, -0.0655, -0.1460,\n",
      "          0.0527,  0.0210, -0.1137,  0.1700, -0.0954,  0.1316,  0.1401, -0.0359],\n",
      "        [-0.1046,  0.0539,  0.2079, -0.0330, -0.0697, -0.1437, -0.1125,  0.1334,\n",
      "          0.0380, -0.0841,  0.0139, -0.1178,  0.1203,  0.1823, -0.0252,  0.0306,\n",
      "         -0.0586,  0.0450,  0.0185, -0.0038, -0.0953,  0.0843, -0.0659, -0.1453,\n",
      "          0.0501,  0.0210, -0.1127,  0.1700, -0.0963,  0.1331,  0.1385, -0.0358]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_dim = 32\n",
    "hidden_dim = 64\n",
    "latent_dim = 16\n",
    "num_embeddings = 128\n",
    "commitment_cost = 0.25\n",
    "\n",
    "model = VQVAE(in_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)\n",
    "x = torch.randn(8, in_dim)  # batch of data\n",
    "x_recon, vq_loss, recon_loss, encoding_indices = model(x)\n",
    "total_loss = recon_loss + vq_loss\n",
    "\n",
    "print(f\"Reconstruction loss: {recon_loss.item()}\")\n",
    "print(f\"VQ loss (commitment + codebook): {vq_loss.item()}\")\n",
    "print(f\"Total loss:{total_loss.item()}\")\n",
    "print(f\"Encoding indices (codebook ids used): {encoding_indices}\")\n",
    "print()\n",
    "\n",
    "# Suppose we want to sample 4 new examples\n",
    "num_samples = 4\n",
    "sampled_indices = torch.randint(low=0, high=num_embeddings, size=(num_samples,))\n",
    "sampled_embeddings = model.vq.embeddings(sampled_indices)\n",
    "generated = model.decoder(sampled_embeddings)\n",
    "print(f\"Sampled embeddings: {sampled_embeddings}\")\n",
    "print()\n",
    "print(f\"Generated samples: {generated}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34178dd2-576d-4cba-922d-a8e2f8c775e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
